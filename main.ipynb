{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import StyleTransferDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple style transfer\n",
    "\n",
    "In this section we will directly apply the style transfer to the image by treating the image pixels as weights and optimizing them. First we will define all the hyperparameters for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "\"CONTENT_IMAGE_PATH\" : \"./frame0.jpg\",\n",
    "\"STYLE_IMAGE_PATH\" : \"./anime.jpg\",\n",
    "\"STYLE_WEIGHT\" : 100000.0,\n",
    "\"LOSS_CONTENT\" : content_gatyes,\n",
    "\"LOSS_STYLE\" : style_gatyes,\n",
    "\"CONTENT_LAYERS\" : [6],\n",
    "\"CONTENT_LAYERS_WEIGHTS\" : [1.0],\n",
    "\"STYLE_LAYERS\" : [8,11],\n",
    "\"STYLE_LAYERS_WEIGHTS\" : (1.0,1.0,1.0,1.0),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we load our content and style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = ToTensor()(Image.open(hp[\"CONTENT_IMAGE_PATH\"]).convert('RGB').resize((512,512)))\n",
    "style_image = ToTensor()(Image.open(hp[\"STYLE_IMAGE_PATH\"]).convert('RGB').resize((512,512)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the model. We will use the standard vgg19 model by pytorch. We will use the model without the classification head and add a normalization layer to match the distribution of the models training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (20): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (30): ReLU(inplace=True)\n",
       "  (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (36): ReLU(inplace=True)\n",
       "  (37): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "# lets print the model\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Model layer: 0 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "  )\n",
       "  (Model layer: 1 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 2 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 3 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 4 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 5 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Model layer: 6 | Content layer: True | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 7 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 8 | Content layer: False | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 9 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 10 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Model layer: 11 | Content layer: False | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_loss_model = construct_style_loss_model(vgg,hp[\"CONTENT_LAYERS\"],hp[\"STYLE_LAYERS\"])\n",
    "style_loss_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to eval just in case it contains e.g. Dropout layers\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "\n",
    "# lets bring everything to the correct device\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "content_image = content_image.to(device)\n",
    "style_image = style_image.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set our initial image to the content image. As an optimizer we will use LBFGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nn.Parameter(content_image.clone().to(device))\n",
    "optimizer = optim.LBFGS([img],lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we precompute the features of the content and style images\n",
    "with torch.no_grad():\n",
    "    _,content_features_target,_ = style_loss_model((content_image.unsqueeze(0),[],[]))\n",
    "    _,_,style_features_target = style_loss_model((style_image.unsqueeze(0),[],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(): \n",
    "\n",
    "    with torch.no_grad():\n",
    "        img.clamp_(0, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    _,content_features,style_features= style_loss_model((img.unsqueeze(0),[],[]))\n",
    "\n",
    "    content_loss = 0.0\n",
    "    for f,f_target in zip(content_features,content_features_target):\n",
    "        content_loss += content_gatyes(f,f_target).mean()\n",
    "\n",
    "    style_loss = 0.0\n",
    "    for f,f_target in zip(style_features,style_features_target):\n",
    "        style_loss += style_mmd_polynomial(f,f_target).mean()\n",
    "\n",
    "    loss = content_loss+100000.0*style_loss\n",
    "    loss.backward()\n",
    "\n",
    "    return (content_loss+100000.0*style_loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 311/500 [09:59<06:04,  1.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m)):\n\u001b[1;32m----> 3\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(compute_losses)\n\u001b[0;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m         img\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[0;32m    434\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 438\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[0;32m    439\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    440\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m, in \u001b[0;36mcompute_losses\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[39m=\u001b[39m content_loss\u001b[39m+\u001b[39m\u001b[39m100000.0\u001b[39m\u001b[39m*\u001b[39mstyle_loss\n\u001b[0;32m     18\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m (content_loss\u001b[39m+\u001b[39;49m\u001b[39m100000.0\u001b[39;49m\u001b[39m*\u001b[39;49mstyle_loss)\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(500)):\n",
    "\n",
    "    optimizer.step(compute_losses)\n",
    "    with torch.no_grad():\n",
    "        img.clamp_(0, 1)\n",
    "    pil = ToPILImage()(img.squeeze(0))\n",
    "    pil.save(\"./result.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feedforward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_encoder = vgg[:14]\n",
    "decoder = construct_decoder_from_encoder(vgg_encoder.cpu(),3,512,512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
