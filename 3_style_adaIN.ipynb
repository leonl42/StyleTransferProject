{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import LoadCoCoDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder,AdaIN\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN\n",
    "from utils.utility import video_to_frame_generator,video_to_frames,normalize\n",
    "import cv2\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image,ImageOps\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "IMAGE_SIZE = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_LAYERS = [6]\n",
    "STYLE_LAYERS = [9,12,14]\n",
    "CONTENT_LAYERS_WEIGHTS = [1.0]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 15)\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# load standard vgg19 model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "style_loss_model = construct_style_loss_model(vgg,CONTENT_LAYERS,STYLE_LAYERS)\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "\n",
    "# as an encoder for our feedforward model we will just use the already existing vgg layers up to some point\n",
    "vgg_encoder = vgg[:8]\n",
    "\n",
    "# based on that we build a decoder that reverses our encoder and matches the shapes through interpolation\n",
    "decoder = construct_decoder_from_encoder(vgg_encoder.cpu(),3,*IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "class StyleModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.adain = AdaIN()\n",
    "\n",
    "    def forward(self, image, image_style, alpha = 1.0):\n",
    "        # alpha creates a weighted average between image and style_image features\n",
    "        # note that this should not be done during training, but only during inference when controlling the style level\n",
    "        f = self.encoder(image)\n",
    "        f_style = self.encoder(image_style)\n",
    "\n",
    "        \n",
    "        f = (1-alpha)*f + alpha*self.adain(f,f_style)\n",
    "\n",
    "        image = self.decoder(f)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "style_model = StyleModel(vgg_encoder,decoder).to(device)\n",
    "optimimizer = optim.Adam(decoder.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"./test2017/\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\"\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataloader = torch.utils.data.DataLoader(LoadCoCoDataset(COCO_PATH, BATCH_SIZE ,IMAGE_SIZE),batch_size=BATCH_SIZE)\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).unsqueeze(0).repeat(BATCH_SIZE,1,1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "EPOCHS = 10000\n",
    "ACCUM_GRAD = 1\n",
    "PRINT_STATS_EVERY = 500\n",
    "STYLE_WEIGHT = 10.0\n",
    "LOSS_CONTENT = content_gatyes\n",
    "LOSS_STYLE = adaIN\n",
    "SAVE_PATH = \"./adain_save/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/10000 [01:55<40:31,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  500  with content loss:  1.6700294017791748  and style loss:  1.5538650751113892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1001/10000 [03:49<37:04,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1000  with content loss:  1.6408993005752563  and style loss:  1.0866953134536743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1501/10000 [05:43<36:48,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1500  with content loss:  1.619925618171692  and style loss:  0.8881847858428955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2001/10000 [07:37<34:04,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2000  with content loss:  1.6041851043701172  and style loss:  0.7736931443214417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2501/10000 [09:33<32:07,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2500  with content loss:  1.5910303592681885  and style loss:  0.6981415748596191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3001/10000 [11:27<28:39,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3000  with content loss:  1.579669713973999  and style loss:  0.6427444815635681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3501/10000 [13:19<26:41,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3500  with content loss:  1.5697897672653198  and style loss:  0.6003834009170532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4001/10000 [15:14<25:08,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4000  with content loss:  1.5606598854064941  and style loss:  0.5665675401687622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4501/10000 [17:06<23:16,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4500  with content loss:  1.5525904893875122  and style loss:  0.5390615463256836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/10000 [18:57<20:53,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5000  with content loss:  1.545293927192688  and style loss:  0.5163284540176392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5501/10000 [20:47<18:18,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5500  with content loss:  1.538694977760315  and style loss:  0.4970754086971283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6001/10000 [22:35<16:06,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6000  with content loss:  1.5325473546981812  and style loss:  0.4804943799972534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6501/10000 [24:22<13:40,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6500  with content loss:  1.5268853902816772  and style loss:  0.46600043773651123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7001/10000 [26:09<12:17,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7000  with content loss:  1.5217007398605347  and style loss:  0.4534367024898529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7501/10000 [27:55<09:45,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7500  with content loss:  1.5168037414550781  and style loss:  0.4419664740562439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8001/10000 [29:40<07:51,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  8000  with content loss:  1.5123587846755981  and style loss:  0.4318700432777405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8501/10000 [31:25<05:59,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  8500  with content loss:  1.5082148313522339  and style loss:  0.422717809677124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9001/10000 [33:12<03:53,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  9000  with content loss:  1.5042678117752075  and style loss:  0.4143603444099426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9501/10000 [35:08<02:05,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  9500  with content loss:  1.5007281303405762  and style loss:  0.40696725249290466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [36:59<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "content_loss_aggregator = []\n",
    "style_loss_aggregator = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    optimimizer.zero_grad()\n",
    "\n",
    "    for _ in range(ACCUM_GRAD):\n",
    "        content_img = next(iter(coco_dataloader)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _,content_features_target,_ = style_loss_model((content_img,[],[]))\n",
    "            _,_,style_features_target = style_loss_model((style_img,[],[]))\n",
    "        \n",
    "\n",
    "        prediction = style_model(content_img,style_img)\n",
    "\n",
    "        _,content_features,style_features= style_loss_model((prediction,[],[]))\n",
    "\n",
    "        content_loss = 0.0\n",
    "        for f,f_target,weight in zip(content_features,content_features_target, CONTENT_LAYERS_WEIGHTS):\n",
    "            content_loss += weight*LOSS_CONTENT(*normalize(f,f_target)).mean()\n",
    "\n",
    "        style_loss = 0.0\n",
    "        for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "            style_loss += weight*LOSS_STYLE(*normalize(f,f_target)).mean()\n",
    "\n",
    "        style_loss *= STYLE_WEIGHT\n",
    "\n",
    "        content_loss_aggregator.append(content_loss.detach().cpu())\n",
    "        style_loss_aggregator.append(style_loss.detach().cpu())\n",
    "\n",
    "        loss = content_loss+style_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    optimimizer.step()  \n",
    "\n",
    "    if epoch%PRINT_STATS_EVERY == 0 and not epoch == 0:\n",
    "        print(\"Ending epoch: \", str(epoch), \" with content loss: \", torch.stack(content_loss_aggregator).mean().numpy().item(),  \" and style loss: \", torch.stack(style_loss_aggregator).mean().numpy().item())\n",
    "        ToPILImage()(content_img[0].permute(0,2,1)).save(SAVE_PATH + \"example_input_model.jpg\")\n",
    "        with torch.no_grad():   \n",
    "            img = style_model(content_img[0].unsqueeze(0), style_img[0].unsqueeze(0))\n",
    "        img = ToPILImage()(img.cpu().squeeze(0).permute(0,2,1))\n",
    "        img.save(SAVE_PATH + \"example_output_model.jpg\")\n",
    "        torch.save(style_model.state_dict(), SAVE_PATH + \"model_weights.pth\")\n",
    "        torch.save(optimimizer.state_dict(), SAVE_PATH + \"optim_weights.pth\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "style_model.load_state_dict(torch.load(SAVE_PATH + \"model_weights.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_IMAGE_PATH = \"./dragon.jpg\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_img = Image.open(CONTENT_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "content_img = ImageOps.fit(content_img,(min(content_img.size),min(content_img.size))).resize(IMAGE_SIZE)\n",
    "content_img = ToTensor()(content_img).permute(0,2,1).to(device)\n",
    "\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# controlls the \"amount of style transfer\"\n",
    "ALPHA = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_style_image = style_model(content_img.unsqueeze(0),style_img.unsqueeze(0), ALPHA).squeeze(0).cpu()\n",
    "content_style_image = ToPILImage()(content_style_image.permute(0,2,1))\n",
    "content_style_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
