{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import StyleTransferDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN,style_mmd_gaussian\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple style transfer\n",
    "\n",
    "In this section we will directly apply the style transfer to the image by treating the image pixels as weights and optimizing them. First we will load our content and our style image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_IMAGE_PATH = \"./frame0.jpg\"\n",
    "STYLE_IMAGE_PATH = \"./anime.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = ToTensor()(Image.open(CONTENT_IMAGE_PATH).convert('RGB').resize((512,512)))\n",
    "style_image = ToTensor()(Image.open(STYLE_IMAGE_PATH).convert('RGB').resize((512,512)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the model. We will use the standard vgg19 model by pytorch. We will use the model without the classification head and add a normalization layer to match the distribution of the models training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (20): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (30): ReLU(inplace=True)\n",
       "  (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (36): ReLU(inplace=True)\n",
       "  (37): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "# lets print the model\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we define which layers we will use as content and weight layers. Note that the indeces match the indices of the printed vgg model. \n",
    "# So index (6) means using the layer \"Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\".\n",
    "# Note that in theory not only conv layers can be used. Some papers also use the ReLU layers between conv.\n",
    "CONTENT_LAYERS = [6]\n",
    "STYLE_LAYERS = [8,11]\n",
    "\n",
    "# Each layers gets a weighting. Default is just 1.0 for every layer. Note that these lists have to have the same length as the lists for choosing the layers.\n",
    "CONTENT_LAYERS_WEIGHTS = [1.0]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0]\n",
    "\n",
    "if not len(CONTENT_LAYERS) == len(CONTENT_LAYERS_WEIGHTS):\n",
    "    raise AssertionError(\"CONTENT_LAYERS and CONTENT_LAYERS_WEIGHTS have to have the same length but were {0} and {1} respectively\".format(len(CONTENT_LAYERS),len(CONTENT_LAYERS_WEIGHTS)))\n",
    "if not len(STYLE_LAYERS) == len(STYLE_LAYERS_WEIGHTS):\n",
    "    raise AssertionError(\"STYLE_LAYERS and STYLE_LAYERS_WEIGHTS have to have the same length but were {0} and {1} respectively\".format(len(STYLE_LAYERS),len(STYLE_LAYERS_WEIGHTS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Model layer: 0 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "  )\n",
       "  (Model layer: 1 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 2 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 3 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 4 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 5 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Model layer: 6 | Content layer: True | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 7 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 8 | Content layer: False | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 9 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 10 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Model layer: 11 | Content layer: False | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on these information we construct our style loss model. As input it will take a tuple containing an image and two empty lists.\n",
    "# It will return a tuple containing the output and two lists containing the content and style features respectively.\n",
    "style_loss_model = construct_style_loss_model(vgg,CONTENT_LAYERS,STYLE_LAYERS)\n",
    "style_loss_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to eval just in case it contains e.g. Dropout layers\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "\n",
    "# lets bring everything to the correct device\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "content_image = content_image.to(device)\n",
    "style_image = style_image.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set our initial image to the content image. As an optimizer we will use LBFGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm yields better results if we set the initial image to the content image\n",
    "img = nn.Parameter(content_image.clone().to(device))\n",
    "optimizer = optim.LBFGS([img],lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we precompute the content and style features of the content and style images respectively\n",
    "with torch.no_grad():\n",
    "    _,content_features_target,_ = style_loss_model((content_image.unsqueeze(0),[],[]))\n",
    "    _,_,style_features_target = style_loss_model((style_image.unsqueeze(0),[],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_WEIGHT = 100000.0\n",
    "LOSS_CONTENT = content_gatyes\n",
    "LOSS_STYLE = style_gatyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(): \n",
    "\n",
    "    with torch.no_grad():\n",
    "        img.clamp_(0, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    _,content_features,style_features= style_loss_model((img.unsqueeze(0),[],[]))\n",
    "\n",
    "    content_loss = 0.0\n",
    "    for f,f_target,weight in zip(content_features,content_features_target, CONTENT_LAYERS_WEIGHTS):\n",
    "        content_loss += weight*LOSS_CONTENT(f,f_target).mean()\n",
    "\n",
    "    style_loss = 0.0\n",
    "    for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "        style_loss += LOSS_STYLE(f,f_target).mean()\n",
    "\n",
    "    loss = content_loss+STYLE_WEIGHT*style_loss\n",
    "    loss.backward()\n",
    "\n",
    "    return (content_loss+STYLE_WEIGHT*style_loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save the current image\n",
    "SAVING_PATH = \"./result.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(3)):\n",
    "\n",
    "    optimizer.step(compute_losses)\n",
    "    with torch.no_grad():\n",
    "        img.clamp_(0, 1)\n",
    "    pil = ToPILImage()(img.squeeze(0))\n",
    "    pil.show()\n",
    "    pil.save(SAVING_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eb3df79a00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(pil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
