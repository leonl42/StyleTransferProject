{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple style transfer\n",
    "\n",
    "In this section we will directly apply the style transfer to the image by treating the image pixels as weights and optimizing them. First we will load our content and our style image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN,style_mmd_gaussian\n",
    "from utils.utility import normalize,normalize_cw\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image,ImageOps\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_IMAGE_PATH = \"./dragon.jpg\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\"\n",
    "IMAGE_SIZE = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_img = Image.open(CONTENT_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "content_img = ImageOps.fit(content_img,(min(content_img.size),min(content_img.size))).resize(IMAGE_SIZE)\n",
    "# Since PIL has the format [W x H x C], and ToTensor() transforms it into [C x H x W], we have to permute the tensor to shape [C x W x H]\n",
    "content_img = ToTensor()(content_img).permute(0,2,1)\n",
    "\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (20): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (30): ReLU(inplace=True)\n",
       "  (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (36): ReLU(inplace=True)\n",
       "  (37): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# Next we load the model. We will use the standard vgg19 model by pytorch. \n",
    "# We will use the model without the classification head and add a normalization layer to match the distribution of the models training data:\n",
    "# load model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "# lets print the model\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# next we define which layers we will use as content and weight layers. Note that the indeces match the indices of the printed vgg model. \n",
    "# So index (6) means using the layer \"Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\".\n",
    "# Note that in theory not only conv layers can be used. Some papers also use the ReLU layers between conv.\n",
    "CONTENT_LAYERS = [3,6]\n",
    "STYLE_LAYERS = [6,8,11]\n",
    "\n",
    "# Each layers gets a weighting. Default is just 1.0 for every layer. Note that these lists have to have the same length as the lists for choosing the layers.\n",
    "CONTENT_LAYERS_WEIGHTS = [1.0,1.0]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0,1.0]\n",
    "\n",
    "if not len(CONTENT_LAYERS) == len(CONTENT_LAYERS_WEIGHTS):\n",
    "    raise AssertionError(\"CONTENT_LAYERS and CONTENT_LAYERS_WEIGHTS have to have the same length but were {0} and {1} respectively\".format(len(CONTENT_LAYERS),len(CONTENT_LAYERS_WEIGHTS)))\n",
    "if not len(STYLE_LAYERS) == len(STYLE_LAYERS_WEIGHTS):\n",
    "    raise AssertionError(\"STYLE_LAYERS and STYLE_LAYERS_WEIGHTS have to have the same length but were {0} and {1} respectively\".format(len(STYLE_LAYERS),len(STYLE_LAYERS_WEIGHTS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Model layer: 0 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "  )\n",
       "  (Model layer: 1 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 2 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 3 | Content layer: True | Style layer: False): Parallel(\n",
       "    (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 4 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 5 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Model layer: 6 | Content layer: True | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 7 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 8 | Content layer: False | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (Model layer: 9 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): ReLU(inplace=True)\n",
       "  )\n",
       "  (Model layer: 10 | Content layer: False | Style layer: False): Parallel(\n",
       "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Model layer: 11 | Content layer: False | Style layer: True): Parallel(\n",
       "    (layer): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# Based on these information we construct our style loss model. As input it will take a tuple containing an image and two empty lists.\n",
    "# It will return a tuple containing the output and two lists containing the features from the chosen content and style layers respectively.\n",
    "style_loss_model = construct_style_loss_model(vgg,CONTENT_LAYERS,STYLE_LAYERS)\n",
    "style_loss_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# set the model to eval just in case it contains e.g. Dropout layers\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "\n",
    "# lets bring everything to the correct device\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "content_img = content_img.to(device)\n",
    "style_img = style_img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# The algorithm returns better results if we set the initial image to the content image\n",
    "# We could also use random noise: torch.rand_like(content_image)\n",
    "img = nn.Parameter(content_img.clone().contiguous().to(device))\n",
    "optimizer = optim.LBFGS([img],lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# we precompute the content and style features of the content and style images respectively\n",
    "with torch.no_grad():\n",
    "    _,content_features_target,_ = style_loss_model((content_img.unsqueeze(0),[],[]))\n",
    "    _,_,style_features_target = style_loss_model((style_img.unsqueeze(0),[],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "STYLE_WEIGHT = 100000.0\n",
    "LOSS_CONTENT = content_gatyes\n",
    "# Possible values for loss style are style_gatyes,style_mmd_polynomial,style_mmd_gaussian,adaIN\n",
    "# Style_mmd_gaussian does not work well \n",
    "# You might have to lover STYLE_WEIGHT when choosing adaIN\n",
    "LOSS_STYLE = adaIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# LBFGS works a bit different then other pytorch optimizers. It requires a loss function in which the magic happens. Dont worry about it.\n",
    "def compute_losses(): \n",
    "\n",
    "    # Clip all values of the image to the range [0,1]\n",
    "    with torch.no_grad():\n",
    "        img.clamp_(0, 1)\n",
    "\n",
    "    # initialize (reset) optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get the features from the chosen content and style layers for our image\n",
    "    _,content_features,style_features= style_loss_model((img.unsqueeze(0),[],[]))\n",
    "\n",
    "    # calculate loss for every layer and sum it up\n",
    "    content_loss = 0.0\n",
    "    for f,f_target,weight in zip(content_features,content_features_target, CONTENT_LAYERS_WEIGHTS):\n",
    "        content_loss += weight*LOSS_CONTENT(*normalize_cw(f,f_target)).mean()\n",
    "\n",
    "    # calculate loss for every layer and sum it up\n",
    "    style_loss = 0.0\n",
    "    for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "        style_loss += weight*LOSS_STYLE(*normalize(f,f_target)).mean()\n",
    "\n",
    "    style_loss *= STYLE_WEIGHT\n",
    "    \n",
    "    loss = content_loss+style_loss\n",
    "    loss.backward()\n",
    "\n",
    "    return (content_loss+style_loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "\n",
    "    optimizer.step(compute_losses)\n",
    "\n",
    "    # Clip all values of the image to the range [0,1]\n",
    "    with torch.no_grad():\n",
    "        img.clamp_(0, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "img = ToPILImage()(img.squeeze(0).permute(0,2,1))\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
