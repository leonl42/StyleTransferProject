{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import LoadFilesDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN\n",
    "from utils.utility import video_to_frame_generator,video_to_frames,normalize\n",
    "import cv2\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 16:9 ratio\n",
    "IMAGE_SIZE = (512,288)\n",
    "CONTENT_VIDEO_PATH = \"./content_video/content_video.mp4\"\n",
    "STYLE_VIDEO_PATH = \"./style_video/style_video.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a generator that returns each video frame\n",
    "content_frame_generator = video_to_frame_generator(CONTENT_VIDEO_PATH,IMAGE_SIZE)\n",
    "style_frame_generator = video_to_frame_generator(STYLE_VIDEO_PATH,IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_FRAME_SAVE_PATH = \"./content_frames/\"\n",
    "STYLE_FRAME_SAVE_PATH = \"./style_frames/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# save the single frames in a path to load later\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m video_to_frames(content_frame_generator, CONTENT_FRAME_SAVE_PATH)\n\u001b[0;32m      3\u001b[0m video_to_frames(style_frame_generator, STYLE_FRAME_SAVE_PATH)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\Documents\\StyleTransferProject\\utils\\utility.py:27\u001b[0m, in \u001b[0;36mvideo_to_frames\u001b[1;34m(frame_generator, save_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m os\u001b[39m.\u001b[39mmakedirs(save_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i,frame \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(frame_generator):\n\u001b[1;32m---> 27\u001b[0m     frame\u001b[39m.\u001b[39;49msave(save_path \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(i) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\miniconda3\\envs\\StyleTransfer\\lib\\site-packages\\PIL\\Image.py:2431\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2431\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[0;32m   2432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   2433\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\miniconda3\\envs\\StyleTransfer\\lib\\site-packages\\PIL\\JpegImagePlugin.py:806\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[39m# The EXIF info needs to be written as one block, + APP1, + one spare byte.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m# Ensure that our buffer is big enough. Same with the icc_profile block.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m bufsize \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(ImageFile\u001b[39m.\u001b[39mMAXBLOCK, bufsize, \u001b[39mlen\u001b[39m(exif) \u001b[39m+\u001b[39m \u001b[39m5\u001b[39m, \u001b[39mlen\u001b[39m(extra) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 806\u001b[0m ImageFile\u001b[39m.\u001b[39;49m_save(im, fp, [(\u001b[39m\"\u001b[39;49m\u001b[39mjpeg\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)], bufsize)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\miniconda3\\envs\\StyleTransfer\\lib\\site-packages\\PIL\\ImageFile.py:520\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    518\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mfileno()\n\u001b[0;32m    519\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n\u001b[1;32m--> 520\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    522\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;00m, exc)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\miniconda3\\envs\\StyleTransfer\\lib\\site-packages\\PIL\\ImageFile.py:547\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    544\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    545\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m         \u001b[39m# slight speedup: compress to real file object\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m         s \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode_to_file(fh, bufsize)\n\u001b[0;32m    548\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    549\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mencoder error \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m when writing image file\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# save the single frames in a path to load later\n",
    "video_to_frames(content_frame_generator, CONTENT_FRAME_SAVE_PATH)\n",
    "video_to_frames(style_frame_generator, STYLE_FRAME_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_LAYERS = [6]\n",
    "STYLE_LAYERS = [9,12,14]\n",
    "CONTENT_LAYERS_WEIGHTS = [1.0]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 15)\n"
     ]
    }
   ],
   "source": [
    "# load standard vgg19 model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "style_loss_model = construct_style_loss_model(vgg,CONTENT_LAYERS,STYLE_LAYERS)\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "\n",
    "# as an encoder for our feedforward model we will just use the already existing vgg layers up to some point\n",
    "vgg_encoder = vgg[:8]\n",
    "\n",
    "# based on that we build a decoder that reverses our encoder and matches the shapes through interpolation\n",
    "decoder = construct_decoder_from_encoder(vgg_encoder.cpu(),3,*IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dataloader =data.DataLoader(LoadFilesDataset(CONTENT_FRAME_SAVE_PATH,batch_size= BATCH_SIZE),batch_size=BATCH_SIZE)\n",
    "style_dataloader = data.DataLoader(LoadFilesDataset(STYLE_FRAME_SAVE_PATH,batch_size= BATCH_SIZE),batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model = nn.Sequential(vgg_encoder,decoder).to(device)\n",
    "optimimizer = optim.Adam(decoder.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "ACCUM_GRAD = 1\n",
    "PRINT_STATS_EVERY = 500\n",
    "STYLE_WEIGHT = 1000000.0\n",
    "LOSS_CONTENT = content_gatyes\n",
    "LOSS_STYLE = style_gatyes\n",
    "SAVE_PATH = \"./feedforward_save/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/10000 [04:10<1:21:32,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  500  with content loss:  1.7328954935073853  and style loss:  5.0672783851623535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1001/10000 [07:57<1:16:14,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1000  with content loss:  1.673820972442627  and style loss:  4.473562717437744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1501/10000 [11:43<1:11:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1500  with content loss:  1.6176385879516602  and style loss:  4.264816761016846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2001/10000 [15:30<1:07:42,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2000  with content loss:  1.5646336078643799  and style loss:  4.079663276672363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2501/10000 [19:15<1:02:57,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2500  with content loss:  1.5181852579116821  and style loss:  4.000204563140869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3001/10000 [23:00<58:28,  1.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3000  with content loss:  1.4766038656234741  and style loss:  3.906033515930176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3501/10000 [26:46<54:04,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3500  with content loss:  1.4398655891418457  and style loss:  3.8338711261749268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4001/10000 [30:36<50:15,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4000  with content loss:  1.4078761339187622  and style loss:  3.775028705596924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4501/10000 [34:25<47:53,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4500  with content loss:  1.379982590675354  and style loss:  3.7367610931396484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/10000 [38:17<41:11,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5000  with content loss:  1.3554295301437378  and style loss:  3.7128803730010986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5501/10000 [42:05<37:42,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5500  with content loss:  1.3334014415740967  and style loss:  3.6720876693725586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5559/10000 [42:31<33:58,  2.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ACCUM_GRAD):\n\u001b[1;32m---> 11\u001b[0m     content_img \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(content_dataloader))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     style_img \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(style_dataloader))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "content_loss_aggregator = []\n",
    "style_loss_aggregator = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    optimimizer.zero_grad()\n",
    "\n",
    "    for _ in range(ACCUM_GRAD):\n",
    "        content_img = next(iter(content_dataloader)).to(device)\n",
    "        style_img = next(iter(style_dataloader)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _,content_features_target,_ = style_loss_model((content_img,[],[]))\n",
    "            _,_,style_features_target = style_loss_model((style_img,[],[]))\n",
    "        \n",
    "\n",
    "        prediction = style_model(content_img)\n",
    "\n",
    "        _,content_features,style_features= style_loss_model((prediction,[],[]))\n",
    "\n",
    "        content_loss = 0.0\n",
    "        for f,f_target,weight in zip(content_features,content_features_target, CONTENT_LAYERS_WEIGHTS):\n",
    "            content_loss += weight*LOSS_CONTENT(normalize(f,f_target)).mean()\n",
    "\n",
    "        style_loss = 0.0\n",
    "        for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "            style_loss += weight*LOSS_STYLE(normalize(f,f_target)).mean()\n",
    "\n",
    "        style_loss *= STYLE_WEIGHT\n",
    "\n",
    "        content_loss_aggregator.append(content_loss.detach().cpu())\n",
    "        style_loss_aggregator.append(style_loss.detach().cpu())\n",
    "\n",
    "        loss = content_loss+style_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    optimimizer.step()\n",
    "\n",
    "    if epoch%PRINT_STATS_EVERY == 0 and not epoch == 0:\n",
    "        print(\"Ending epoch: \", str(epoch), \" with content loss: \", torch.stack(content_loss_aggregator).mean().numpy().item(),  \" and style loss: \", torch.stack(style_loss_aggregator).mean().numpy().item())\n",
    "        img = next(iter(content_dataloader))[0]\n",
    "        ToPILImage()(img.permute(0,2,1)).save(SAVE_PATH + \"example_input_model.jpg\")\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            img = style_model(img)\n",
    "        img = ToPILImage()(img.cpu().squeeze(0).permute(0,2,1))\n",
    "        img.save(SAVE_PATH + \"example_output_model.jpg\")\n",
    "        torch.save(style_model.state_dict(), SAVE_PATH + \"model_weights.pth\")\n",
    "        torch.save(optimimizer.state_dict(), SAVE_PATH + \"optim_weights.pth\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model.load_state_dict(torch.load(SAVE_PATH + \"model_weights.pth\",map_location=device))\n",
    "optimimizer.load_state_dict(torch.load(SAVE_PATH + \"optim_weights.pth\",map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
