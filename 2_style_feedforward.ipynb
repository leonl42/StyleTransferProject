{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import LoadCoCoDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN\n",
    "from utils.utility import video_to_frame_generator,video_to_frames,normalize\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.datasets import CocoDetection\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image,ImageOps\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "IMAGE_SIZE = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_LAYERS = [6]\n",
    "STYLE_LAYERS = [9,12,14]\n",
    "CONTENT_LAYERS_WEIGHTS = [1.0]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# load standard vgg19 model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "style_loss_model = construct_style_loss_model(vgg,CONTENT_LAYERS,STYLE_LAYERS)\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "\n",
    "# as an encoder for our feedforward model we will just use the already existing vgg layers up to some point\n",
    "vgg_encoder = vgg[:8]\n",
    "\n",
    "# based on that we build a decoder that reverses our encoder and matches the shapes through interpolation\n",
    "decoder = construct_decoder_from_encoder(vgg_encoder.cpu(),3,*IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"./test2017/\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\"\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataloader = torch.utils.data.DataLoader(LoadCoCoDataset(COCO_PATH, BATCH_SIZE ,IMAGE_SIZE),batch_size=BATCH_SIZE)\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).unsqueeze(0).repeat(BATCH_SIZE,1,1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "style_model = nn.Sequential(vgg_encoder,decoder).to(device)\n",
    "optimimizer = optim.Adam(decoder.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "EPOCHS = 10000\n",
    "ACCUM_GRAD = 1\n",
    "PRINT_STATS_EVERY = 500\n",
    "STYLE_WEIGHT = 10000.0\n",
    "LOSS_CONTENT = content_gatyes\n",
    "LOSS_STYLE = style_gatyes\n",
    "SAVE_PATH = \"./feedforward_save/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "content_loss_aggregator = []\n",
    "style_loss_aggregator = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    optimimizer.zero_grad()\n",
    "\n",
    "    # Since we might have to work with small batch_sizes, simulate big batch sizes by accumulating gradients over multiple batches\n",
    "    for _ in range(ACCUM_GRAD):\n",
    "        content_img = next(iter(coco_dataloader)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _,content_features_target,_ = style_loss_model((content_img,[],[]))\n",
    "            _,_,style_features_target = style_loss_model((style_img,[],[]))\n",
    "        \n",
    "\n",
    "        prediction = style_model(content_img)\n",
    "\n",
    "        # get the features from the content and style layers of our prediction\n",
    "        _,content_features,style_features= style_loss_model((prediction,[],[]))\n",
    "\n",
    "        content_loss = 0.0\n",
    "        for f,f_target,weight in zip(content_features,content_features_target, CONTENT_LAYERS_WEIGHTS):\n",
    "            content_loss += weight*LOSS_CONTENT(f,f_target).mean()\n",
    "\n",
    "        style_loss = 0.0\n",
    "        for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "            style_loss += weight*LOSS_STYLE(f,f_target).mean()\n",
    "\n",
    "        style_loss *= STYLE_WEIGHT\n",
    "\n",
    "        content_loss_aggregator.append(content_loss.detach().cpu())\n",
    "        style_loss_aggregator.append(style_loss.detach().cpu())\n",
    "\n",
    "        loss = content_loss+style_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    optimimizer.step()\n",
    "\n",
    "    # Every PRINT_STATS_EVERY epochs we want to print the current performance, save the weights, and also save an example prediction\n",
    "    if epoch%PRINT_STATS_EVERY == 0 and not epoch == 0:\n",
    "        print(\"Ending epoch: \", str(epoch), \" with content loss: \", torch.stack(content_loss_aggregator).mean().numpy().item(),  \" and style loss: \", torch.stack(style_loss_aggregator).mean().numpy().item())\n",
    "        img = next(iter(coco_dataloader))[0]\n",
    "        ToPILImage()(img.permute(0,2,1)).save(SAVE_PATH + \"example_input_model.jpg\")\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            img = style_model(img)\n",
    "        img = ToPILImage()(img.cpu().squeeze(0).permute(0,2,1))\n",
    "        img.save(SAVE_PATH + \"example_output_model.jpg\")\n",
    "        torch.save(style_model.state_dict(), SAVE_PATH + \"model_weights.pth\")\n",
    "        torch.save(optimimizer.state_dict(), SAVE_PATH + \"optim_weights.pth\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "style_model.load_state_dict(torch.load(SAVE_PATH + \"model_weights.pth\",map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_IMAGE_PATH = \"./dragon.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_img = Image.open(CONTENT_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "content_img = ImageOps.fit(content_img,(min(content_img.size),min(content_img.size))).resize(IMAGE_SIZE)\n",
    "content_img = ToTensor()(content_img).permute(0,2,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_style_image = style_model(content_img.unsqueeze(0)).squeeze(0).cpu()\n",
    "content_style_image = ToPILImage()(content_style_image.permute(0,2,1))\n",
    "content_style_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
