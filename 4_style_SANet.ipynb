{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import LoadCoCoDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder,AdaIN,SANet\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN\n",
    "from utils.utility import video_to_frame_generator,video_to_frames,normalize,normalize_cw\n",
    "import cv2\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image,ImageOps\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "IMAGE_SIZE = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "STYLE_LAYERS = [9,12,14]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 15)\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# load standard vgg19 model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "style_loss_model = construct_style_loss_model(vgg,[],STYLE_LAYERS)\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "\n",
    "# as encoders for our feedforward model we will just use the already existing vgg layers up to some point\n",
    "vgg_encoder_r_4_1 = vgg[:10]\n",
    "vgg_encoder_r_5_1 = vgg[10:13]\n",
    "\n",
    "example_input = torch.rand(16,3,*IMAGE_SIZE)\n",
    "# get the size of the respective encoder outputs\n",
    "size_r_4_1 = vgg_encoder_r_4_1(example_input).size()[1:]\n",
    "size_r_5_1 = vgg_encoder_r_5_1(vgg_encoder_r_4_1(example_input)).size()[1:]\n",
    "\n",
    "# Our Style Model will use the feature output of vgg_encoder_r_5_1 and vgg_encoder_r_4_1. But the vgg_encoder_r_5_1 will be upsampled to the size of vgg_encoder_r_4_1.\n",
    "# After that a 3x3 convolution is applied and that is fed into our decoder. To make our decoder match that last 3x3 convolution size, \n",
    "# we simply add it to the vgg_encoder_r_4_1 when constructing the decoder (NOT IN THE ACTUAL MODEL DIRECTLY AFTER THE ENCODER).\n",
    "# This simulated encoder is then used to construct the decoder. \n",
    "simulated_encoder = nn.Sequential(*vgg[0:10],nn.Conv2d(size_r_4_1[0],size_r_4_1[0],kernel_size=3,stride=3))\n",
    "# based on that we build a decoder that reverses our encoder and matches the shapes through interpolation\n",
    "decoder = construct_decoder_from_encoder(simulated_encoder,3,*IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This encoder returns two feature maps from an encoder. The encoders given as arguments have to be sliced in such a way that the output from encoder 1 can be fed into encoder 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vgg_encoder_r_4_1, vgg_encoder_r_5_1):\n",
    "        super().__init__()\n",
    "        self.vgg_encoder_r_4_1 = vgg_encoder_r_4_1\n",
    "        self.vgg_encoder_r_5_1 = vgg_encoder_r_5_1\n",
    "\n",
    "    def forward(self, img):\n",
    "        f_r_4_1 = self.vgg_encoder_r_4_1(img)\n",
    "        f_r_5_1 = self.vgg_encoder_r_5_1(f_r_4_1)\n",
    "\n",
    "        return f_r_4_1,f_r_5_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "class StyleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementing the full Style Network from the SANet paper. Note that I named the variables after the variables in the model sketch in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, size_r_4_1, size_r_5_1,decoder) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.size_r_4_1 = size_r_4_1\n",
    "\n",
    "        # initialize SANet with the channel size\n",
    "        self.SANet_r_4_1 = SANet(size_r_4_1[0])\n",
    "        self.SANet_r_5_1 = SANet(size_r_5_1[0])\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.conv1 = nn.Conv2d(size_r_4_1[0],size_r_4_1[0],kernel_size=1,stride=1)\n",
    "        self.conv2 = nn.Conv2d(size_r_5_1[0],size_r_5_1[0],kernel_size=1,stride=1)\n",
    "        self.conv3 = nn.Conv2d(size_r_4_1[0],size_r_4_1[0],kernel_size=3,stride=3)\n",
    "\n",
    "    def forward(self, img, img_style):\n",
    "\n",
    "        f_c_r_4_1,f_c_r_5_1 = self.encoder(img)\n",
    "        f_s_r_4_1,f_s_r_5_1 = self.encoder(img_style)\n",
    "\n",
    "        f_cs_r_4_1 = self.conv1(self.SANet_r_4_1(f_c_r_4_1,f_s_r_4_1))\n",
    "        f_cs_r_5_1 = self.conv2(self.SANet_r_5_1(f_c_r_5_1,f_s_r_5_1))\n",
    "\n",
    "        f_csc_r_4_1 = f_cs_r_4_1 + f_c_r_4_1\n",
    "        f_csc_r_5_1 = f_cs_r_5_1 + f_c_r_5_1\n",
    "\n",
    "        f_csc_r_5_1 = F.interpolate(f_csc_r_5_1.unsqueeze(1),self.size_r_4_1).squeeze(1)\n",
    "\n",
    "        f_csc_m = f_csc_r_4_1 + f_csc_r_5_1\n",
    "\n",
    "        f_csc_m = self.conv3(f_csc_m)\n",
    "\n",
    "        return self.decoder(f_csc_m),f_c_r_4_1,f_c_r_5_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "encoder = Encoder(vgg_encoder_r_4_1, vgg_encoder_r_5_1).to(device)\n",
    "encoder.requires_grad_(False)\n",
    "\n",
    "style_model = StyleModel(encoder, size_r_4_1, size_r_5_1,decoder).to(device)\n",
    "style_model.encoder.requires_grad_(False)\n",
    "\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "\n",
    "optimimizer = optim.Adam(style_model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"./test2017/\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\"\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataloader = torch.utils.data.DataLoader(LoadCoCoDataset(COCO_PATH, BATCH_SIZE ,IMAGE_SIZE),batch_size=BATCH_SIZE)\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).unsqueeze(0).repeat(BATCH_SIZE,1,1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "EPOCHS = 2500\n",
    "ACCUM_GRAD = 8\n",
    "PRINT_STATS_EVERY = 100\n",
    "CONTENT_WEIGHT = 25.0\n",
    "STYLE_WEIGHT = 3.0\n",
    "IDENTITY_1_WEIGHT = 50.0\n",
    "IDENTITY_2_WEIGHT = 4000.0\n",
    "SAVE_PATH = \"./SANet_save/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [01:02<1:37:40,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  100  with content loss:  79777.1015625  and style loss:  3.8656041622161865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10000 [02:00<1:36:03,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  200  with content loss:  78774.6953125  and style loss:  3.3525102138519287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [02:59<1:44:57,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  300  with content loss:  78384.921875  and style loss:  3.093285083770752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10000 [03:57<1:29:38,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  400  with content loss:  78125.390625  and style loss:  2.9220762252807617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 500/10000 [04:57<1:33:53,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  500  with content loss:  77896.46875  and style loss:  2.798980236053467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10000 [05:54<1:29:54,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  600  with content loss:  77714.7578125  and style loss:  2.7042291164398193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 700/10000 [06:53<1:31:42,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  700  with content loss:  77585.25  and style loss:  2.629682779312134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 800/10000 [07:52<1:30:52,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  800  with content loss:  77473.140625  and style loss:  2.569742202758789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 900/10000 [08:52<1:30:05,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  900  with content loss:  77349.796875  and style loss:  2.5172319412231445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1000/10000 [09:51<1:28:54,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1000  with content loss:  77254.8515625  and style loss:  2.4737021923065186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1100/10000 [10:50<1:27:49,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1100  with content loss:  77168.1328125  and style loss:  2.4358396530151367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1200/10000 [11:50<1:26:44,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1200  with content loss:  77069.625  and style loss:  2.4037458896636963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1300/10000 [12:49<1:25:46,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1300  with content loss:  76993.6875  and style loss:  2.3749423027038574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1400/10000 [13:48<1:24:26,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1400  with content loss:  76934.75  and style loss:  2.352935552597046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1500/10000 [14:47<1:23:47,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1500  with content loss:  76881.59375  and style loss:  2.330317258834839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1600/10000 [15:46<1:22:31,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1600  with content loss:  76822.828125  and style loss:  2.3106155395507812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1700/10000 [16:45<1:21:36,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1700  with content loss:  76761.7421875  and style loss:  2.2888224124908447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1800/10000 [17:43<1:19:56,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1800  with content loss:  76718.1796875  and style loss:  2.273223876953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1900/10000 [18:43<1:19:43,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  1900  with content loss:  76666.359375  and style loss:  2.2569961547851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2000/10000 [19:42<1:17:57,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2000  with content loss:  76620.8046875  and style loss:  2.242840528488159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2100/10000 [20:41<1:17:25,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2100  with content loss:  76582.9140625  and style loss:  2.228301525115967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2200/10000 [21:40<1:16:30,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2200  with content loss:  76547.3828125  and style loss:  2.2149722576141357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2300/10000 [22:39<1:15:06,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2300  with content loss:  76499.828125  and style loss:  2.202899694442749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2400/10000 [23:38<1:14:28,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2400  with content loss:  76461.1875  and style loss:  2.1929774284362793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2500/10000 [24:37<1:13:55,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2500  with content loss:  76429.5078125  and style loss:  2.1819088459014893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2600/10000 [25:36<1:12:03,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2600  with content loss:  76404.2421875  and style loss:  2.171450138092041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2701/10000 [26:36<1:19:44,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2700  with content loss:  76373.2578125  and style loss:  2.1607820987701416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2800/10000 [27:34<1:10:52,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2800  with content loss:  76337.65625  and style loss:  2.15177059173584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2900/10000 [28:33<1:09:13,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  2900  with content loss:  76310.5703125  and style loss:  2.14178729057312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3001/10000 [29:33<1:15:55,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3000  with content loss:  76278.1875  and style loss:  2.132821559906006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3101/10000 [30:32<1:14:46,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3100  with content loss:  76243.3046875  and style loss:  2.123976230621338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3201/10000 [31:31<1:13:42,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3200  with content loss:  76212.4375  and style loss:  2.115290641784668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3300/10000 [32:29<1:06:13,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3300  with content loss:  76186.078125  and style loss:  2.107769727706909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3400/10000 [33:28<1:04:53,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3400  with content loss:  76161.5078125  and style loss:  2.100367307662964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3500/10000 [34:27<1:03:56,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3500  with content loss:  76136.8828125  and style loss:  2.0928969383239746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3600/10000 [35:27<1:02:45,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3600  with content loss:  76107.8515625  and style loss:  2.085719585418701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3700/10000 [36:26<1:01:50,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3700  with content loss:  76085.1640625  and style loss:  2.0796351432800293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3800/10000 [37:25<1:01:05,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3800  with content loss:  76058.4921875  and style loss:  2.072112560272217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3900/10000 [38:24<58:14,  1.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  3900  with content loss:  76037.234375  and style loss:  2.0658862590789795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4000/10000 [39:19<53:45,  1.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4000  with content loss:  76013.6796875  and style loss:  2.0596718788146973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4101/10000 [40:19<1:04:49,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4100  with content loss:  75994.7890625  and style loss:  2.0534729957580566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4201/10000 [41:17<1:01:43,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4200  with content loss:  75979.0546875  and style loss:  2.0478646755218506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4301/10000 [42:14<58:09,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4300  with content loss:  75953.5390625  and style loss:  2.0424230098724365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4401/10000 [43:12<59:38,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4400  with content loss:  75931.484375  and style loss:  2.035874843597412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4501/10000 [44:10<56:07,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4500  with content loss:  75907.8515625  and style loss:  2.029507875442505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4600/10000 [45:08<54:23,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4600  with content loss:  75883.859375  and style loss:  2.024287700653076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4700/10000 [46:07<53:03,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4700  with content loss:  75865.0859375  and style loss:  2.0188217163085938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4801/10000 [47:04<50:37,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4800  with content loss:  75843.5703125  and style loss:  2.0131027698516846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4900/10000 [47:57<45:12,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  4900  with content loss:  75827.859375  and style loss:  2.010164737701416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/10000 [48:51<49:17,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5000  with content loss:  75807.7109375  and style loss:  2.0056636333465576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5101/10000 [49:45<48:56,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5100  with content loss:  75784.546875  and style loss:  2.001146078109741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5201/10000 [50:38<47:13,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5200  with content loss:  75769.0  and style loss:  1.9974125623703003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5301/10000 [51:32<46:10,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5300  with content loss:  75748.375  and style loss:  1.993502140045166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5400/10000 [52:24<40:26,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5400  with content loss:  75739.265625  and style loss:  1.989772915840149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5500/10000 [53:17<39:41,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5500  with content loss:  75722.2578125  and style loss:  1.9854165315628052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5601/10000 [54:12<43:25,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5600  with content loss:  75710.03125  and style loss:  1.9813228845596313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5701/10000 [55:05<43:14,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5700  with content loss:  75693.5703125  and style loss:  1.9772344827651978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5800/10000 [55:59<37:16,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5800  with content loss:  75676.4375  and style loss:  1.973203182220459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5901/10000 [56:54<40:47,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  5900  with content loss:  75657.7734375  and style loss:  1.9691036939620972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6000/10000 [57:51<40:09,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6000  with content loss:  75647.3359375  and style loss:  1.9657049179077148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6100/10000 [58:48<35:33,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6100  with content loss:  75626.5234375  and style loss:  1.9627845287322998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6201/10000 [59:45<39:16,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6200  with content loss:  75609.5625  and style loss:  1.9597601890563965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 6301/10000 [1:00:41<36:07,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6300  with content loss:  75594.1953125  and style loss:  1.9565483331680298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6401/10000 [1:01:38<37:46,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6400  with content loss:  75577.34375  and style loss:  1.9535638093948364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6501/10000 [1:02:35<36:33,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6500  with content loss:  75559.7578125  and style loss:  1.950373649597168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6601/10000 [1:03:32<35:26,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6600  with content loss:  75539.7421875  and style loss:  1.9473460912704468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6701/10000 [1:04:29<34:26,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6700  with content loss:  75524.0703125  and style loss:  1.9443573951721191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6801/10000 [1:05:26<33:26,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6800  with content loss:  75514.1875  and style loss:  1.9411479234695435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6901/10000 [1:06:23<32:20,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  6900  with content loss:  75503.0625  and style loss:  1.9387032985687256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7001/10000 [1:07:20<31:14,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7000  with content loss:  75491.625  and style loss:  1.9361921548843384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7100/10000 [1:08:18<29:29,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7100  with content loss:  75479.6484375  and style loss:  1.9334336519241333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7201/10000 [1:09:17<29:57,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7200  with content loss:  75470.765625  and style loss:  1.9304673671722412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7301/10000 [1:10:15<28:33,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7300  with content loss:  75454.953125  and style loss:  1.927512764930725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7401/10000 [1:11:12<27:31,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7400  with content loss:  75443.7734375  and style loss:  1.924783706665039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7501/10000 [1:12:10<26:28,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7500  with content loss:  75432.828125  and style loss:  1.9220021963119507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7601/10000 [1:13:08<25:48,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending epoch:  7600  with content loss:  75421.3828125  and style loss:  1.9198262691497803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7695/10000 [1:14:03<22:10,  1.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m i_ss,_,_ \u001b[39m=\u001b[39m style_model(style_img,style_img)\n\u001b[0;32m     36\u001b[0m identity_1_loss \u001b[39m=\u001b[39m IDENTITY_1_WEIGHT\u001b[39m*\u001b[39m(torch\u001b[39m.\u001b[39mnorm((i_cc\u001b[39m-\u001b[39mcontent_img)\u001b[39m.\u001b[39mview(BATCH_SIZE,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mnorm((i_ss\u001b[39m-\u001b[39mstyle_img)\u001b[39m.\u001b[39mview(BATCH_SIZE,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean())\n\u001b[1;32m---> 37\u001b[0m _,_,i_cc_style_features\u001b[39m=\u001b[39m style_loss_model((i_cc,[],[]))\n\u001b[0;32m     38\u001b[0m _,_,content_img_style_features\u001b[39m=\u001b[39m style_loss_model((content_img,[],[]))\n\u001b[0;32m     39\u001b[0m _,_,i_ss_style_features\u001b[39m=\u001b[39m style_loss_model((i_ss,[],[]))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Leon\\Documents\\StyleTransferProject\\utils\\model.py:26\u001b[0m, in \u001b[0;36mParallel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m     23\u001b[0m     \u001b[39m# Input x is a tuple: (<torch.Tensor>, <list>, <list>)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     img,content_list,style_list \u001b[39m=\u001b[39m x\n\u001b[1;32m---> 26\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer(img)\n\u001b[0;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent:\n\u001b[0;32m     28\u001b[0m         content_list\u001b[39m.\u001b[39mappend(img)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    917\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mclone()\n\u001b[0;32m    919\u001b[0m dtype \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> 920\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mas_tensor(mean, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mtensor\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    921\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(std, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    922\u001b[0m \u001b[39mif\u001b[39;00m (std \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "\n",
    "content_loss_aggregator = []\n",
    "style_loss_aggregator = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    optimimizer.zero_grad()\n",
    "\n",
    "    for _ in range(ACCUM_GRAD):\n",
    "        content_img = next(iter(coco_dataloader)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _,_,style_features_target = style_loss_model((style_img,[],[]))\n",
    "        \n",
    "        prediction, f_c_r_4_1, f_c_r_5_1 = style_model(content_img,style_img)\n",
    "\n",
    "        f_c_r_4_1, f_c_r_5_1 = normalize_cw(f_c_r_4_1, f_c_r_5_1)\n",
    "        \n",
    "        pred_cs_r_4_1,pred_cs_r_5_1 = style_model.encoder(prediction)\n",
    "        pred_cs_r_4_1, pred_cs_r_5_1 = normalize_cw(pred_cs_r_4_1, pred_cs_r_5_1)\n",
    "\n",
    "        content_loss = CONTENT_WEIGHT*(torch.norm((pred_cs_r_4_1-f_c_r_4_1).view(BATCH_SIZE,-1), dim=-1).mean() + torch.norm((pred_cs_r_5_1-f_c_r_5_1).view(BATCH_SIZE,-1), dim=-1).mean())\n",
    "\n",
    "        _,_,style_features= style_loss_model((prediction,[],[]))\n",
    "\n",
    "        style_loss = 0.0\n",
    "        for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "            f_mean = f.mean((2,3))\n",
    "            f_std = f.std((2,3))\n",
    "            f_target_mean = f_target.mean((2,3))\n",
    "            f_target_std = f_target.std((2,3))\n",
    "            style_loss += weight*(torch.norm(f_mean - f_target_mean,dim=-1).mean() + torch.norm(f_std - f_target_std,dim=-1).mean())\n",
    "\n",
    "        style_loss *= STYLE_WEIGHT\n",
    "\n",
    "        i_cc,_,_ = style_model(content_img,content_img)\n",
    "        i_ss,_,_ = style_model(style_img,style_img)\n",
    "\n",
    "        identity_1_loss = IDENTITY_1_WEIGHT*(torch.norm((i_cc-content_img).view(BATCH_SIZE,-1), dim=-1).mean() + torch.norm((i_ss-style_img).view(BATCH_SIZE,-1), dim=-1).mean())\n",
    "        _,_,i_cc_style_features= style_loss_model((i_cc,[],[]))\n",
    "        _,_,content_img_style_features= style_loss_model((content_img,[],[]))\n",
    "        _,_,i_ss_style_features= style_loss_model((i_ss,[],[]))\n",
    "        _,_,style_img_style_features= style_loss_model((style_img,[],[]))\n",
    "\n",
    "        identity_2_loss = 0.0\n",
    "        for f_icc,f_ic,f_iss,f_is in zip(i_cc_style_features,content_img_style_features,i_ss_style_features,style_img_style_features):\n",
    "            identity_2_loss += (torch.norm((f_icc-f_ic).view(BATCH_SIZE,-1), dim=-1).mean() + torch.norm((f_iss-f_is).view(BATCH_SIZE,-1), dim=-1).mean())\n",
    "        identity_2_loss *= IDENTITY_2_WEIGHT\n",
    "\n",
    "        content_loss_aggregator.append(content_loss.detach().cpu())\n",
    "        style_loss_aggregator.append(style_loss.detach().cpu())\n",
    "\n",
    "        loss = content_loss + style_loss + identity_1_loss + identity_2_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    optimimizer.step()  \n",
    "\n",
    "    if epoch%PRINT_STATS_EVERY == 0 and not epoch == 0:\n",
    "        print(\"Ending epoch: \", str(epoch), \" with content loss: \", torch.stack(content_loss_aggregator).mean().numpy().item(),  \" and style loss: \", torch.stack(style_loss_aggregator).mean().numpy().item())\n",
    "        ToPILImage()(content_img[0].permute(0,2,1)).save(SAVE_PATH + \"example_input_model.jpg\")\n",
    "        with torch.no_grad():   \n",
    "            img,_,_ = style_model(content_img[0].unsqueeze(0), style_img[0].unsqueeze(0))\n",
    "        img = ToPILImage()(img.cpu().squeeze(0).permute(0,2,1))\n",
    "        img.save(SAVE_PATH + \"example_output_model.jpg\")\n",
    "        torch.save(style_model.state_dict(), SAVE_PATH + \"model_weights.pth\")\n",
    "        torch.save(optimimizer.state_dict(), SAVE_PATH + \"optim_weights.pth\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "style_model.load_state_dict(torch.load(SAVE_PATH + \"model_weights.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_IMAGE_PATH = \"./dragon.jpg\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_img = Image.open(CONTENT_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "content_img = ImageOps.fit(content_img,(min(content_img.size),min(content_img.size))).resize(IMAGE_SIZE)\n",
    "content_img = ToTensor()(content_img).permute(0,2,1).to(device)\n",
    "\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_style_image,_,_ = style_model(content_img.unsqueeze(0),style_img.unsqueeze(0)).squeeze(0).cpu()\n",
    "content_style_image = ToPILImage()(content_style_image.permute(0,2,1))\n",
    "content_style_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
