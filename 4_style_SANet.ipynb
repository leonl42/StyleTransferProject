{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage,Normalize\n",
    "from utils.dataset import LoadCoCoDataset\n",
    "from utils.model import construct_style_loss_model,construct_decoder_from_encoder,AdaIN,SANet\n",
    "from utils.losses import content_gatyes,style_gatyes,style_mmd_polynomial,adaIN\n",
    "from utils.utility import video_to_frame_generator,video_to_frames,normalize,normalize_cw\n",
    "import os\n",
    "from torchvision.models import vgg19,VGG19_Weights\n",
    "from PIL import Image,ImageOps\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"On device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "IMAGE_SIZE = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "STYLE_LAYERS = [3,6,8,11,13]\n",
    "STYLE_LAYERS_WEIGHTS = [1.0,1.0,1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# load standard vgg19 model\n",
    "vgg = vgg19(VGG19_Weights.DEFAULT)\n",
    "\n",
    "# remove classification head\n",
    "vgg = vgg.features\n",
    "\n",
    "# prepend a normalization layer\n",
    "vgg = nn.Sequential(Normalize(mean = (0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)), *vgg)\n",
    "\n",
    "style_loss_model = construct_style_loss_model(vgg,[],STYLE_LAYERS)\n",
    "style_loss_model = style_loss_model.eval()\n",
    "style_loss_model.requires_grad_(False)\n",
    "\n",
    "# as encoders for our feedforward model we will just use the already existing vgg layers up to some point\n",
    "vgg_encoder_r_4_1 = vgg[:10]\n",
    "vgg_encoder_r_5_1 = vgg[10:13]\n",
    "\n",
    "example_input = torch.rand(16,3,*IMAGE_SIZE)\n",
    "# get the size of the respective encoder outputs\n",
    "size_r_4_1 = vgg_encoder_r_4_1(example_input).size()[1:]\n",
    "size_r_5_1 = vgg_encoder_r_5_1(vgg_encoder_r_4_1(example_input)).size()[1:]\n",
    "\n",
    "# Our Style Model will use the feature output of vgg_encoder_r_5_1 and vgg_encoder_r_4_1. But the vgg_encoder_r_5_1 will be upsampled to the size of vgg_encoder_r_4_1.\n",
    "# After that a 3x3 convolution is applied and that is fed into our decoder. To make our decoder match that last 3x3 convolution size, \n",
    "# we simply add it to the vgg_encoder_r_4_1 when constructing the decoder (NOT IN THE ACTUAL MODEL DIRECTLY AFTER THE ENCODER).\n",
    "# This simulated encoder is then used to construct the decoder. \n",
    "simulated_encoder = nn.Sequential(*vgg[0:10],nn.Conv2d(size_r_4_1[0],size_r_4_1[0],kernel_size=3,stride=3))\n",
    "# based on that we build a decoder that reverses our encoder and matches the shapes through interpolation\n",
    "decoder = construct_decoder_from_encoder(simulated_encoder,3,*IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This encoder returns two feature maps from an encoder. The encoders given as arguments have to be sliced in such a way that the output from encoder 1 can be fed into encoder 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vgg_encoder_r_4_1, vgg_encoder_r_5_1):\n",
    "        super().__init__()\n",
    "        self.vgg_encoder_r_4_1 = vgg_encoder_r_4_1\n",
    "        self.vgg_encoder_r_5_1 = vgg_encoder_r_5_1\n",
    "\n",
    "    def forward(self, img):\n",
    "        f_r_4_1 = self.vgg_encoder_r_4_1(img)\n",
    "        f_r_5_1 = self.vgg_encoder_r_5_1(f_r_4_1)\n",
    "\n",
    "        return f_r_4_1,f_r_5_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "class StyleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementing the full Style Network from the SANet paper. Note that I named the variables after the variables in the model sketch in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, size_r_4_1, size_r_5_1,decoder) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.size_r_4_1 = size_r_4_1\n",
    "\n",
    "        # initialize SANet with the channel size\n",
    "        self.SANet_r_4_1 = SANet(size_r_4_1[0])\n",
    "        self.SANet_r_5_1 = SANet(size_r_5_1[0])\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.conv1 = nn.Conv2d(size_r_4_1[0],size_r_4_1[0],kernel_size=1,stride=1)\n",
    "        self.conv2 = nn.Conv2d(size_r_5_1[0],size_r_5_1[0],kernel_size=1,stride=1)\n",
    "        self.conv3 = nn.Conv2d(size_r_4_1[0],size_r_4_1[0],kernel_size=3,stride=3)\n",
    "\n",
    "    def forward(self, img, img_style):\n",
    "\n",
    "        f_c_r_4_1,f_c_r_5_1 = self.encoder(img)\n",
    "        f_s_r_4_1,f_s_r_5_1 = self.encoder(img_style)\n",
    "\n",
    "        f_cs_r_4_1 = self.conv1(self.SANet_r_4_1(f_c_r_4_1,f_s_r_4_1))\n",
    "        f_cs_r_5_1 = self.conv2(self.SANet_r_5_1(f_c_r_5_1,f_s_r_5_1))\n",
    "\n",
    "        f_csc_r_4_1 = f_cs_r_4_1 + f_c_r_4_1\n",
    "        f_csc_r_5_1 = f_cs_r_5_1 + f_c_r_5_1\n",
    "\n",
    "        f_csc_r_5_1 = F.interpolate(f_csc_r_5_1.unsqueeze(1),self.size_r_4_1).squeeze(1)\n",
    "\n",
    "        f_csc_m = f_csc_r_4_1 + f_csc_r_5_1\n",
    "\n",
    "        f_csc_m = self.conv3(f_csc_m)\n",
    "\n",
    "        return self.decoder(f_csc_m),f_c_r_4_1,f_c_r_5_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "encoder = Encoder(vgg_encoder_r_4_1, vgg_encoder_r_5_1).to(device)\n",
    "encoder.requires_grad_(False)\n",
    "\n",
    "style_model = StyleModel(encoder, size_r_4_1, size_r_5_1,decoder).to(device)\n",
    "style_model.encoder.requires_grad_(False)\n",
    "\n",
    "style_loss_model = style_loss_model.to(device)\n",
    "\n",
    "optimimizer = optim.Adam(style_model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"./test2017/\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\"\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataloader = torch.utils.data.DataLoader(LoadCoCoDataset(COCO_PATH, BATCH_SIZE ,IMAGE_SIZE),batch_size=BATCH_SIZE)\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).unsqueeze(0).repeat(BATCH_SIZE,1,1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "EPOCHS = 2500\n",
    "ACCUM_GRAD = 8\n",
    "PRINT_STATS_EVERY = 100\n",
    "CONTENT_WEIGHT = 1.0\n",
    "STYLE_WEIGHT = 30.0\n",
    "IDENTITY_1_WEIGHT = 50.0\n",
    "IDENTITY_2_WEIGHT = 4000.0\n",
    "SAVE_PATH = \"./SANet_save/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "\n",
    "content_loss_aggregator = []\n",
    "style_loss_aggregator = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    optimimizer.zero_grad()\n",
    "\n",
    "    for _ in range(ACCUM_GRAD):\n",
    "        content_img = next(iter(coco_dataloader)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _,_,style_features_target = style_loss_model((style_img,[],[]))\n",
    "        \n",
    "        prediction, f_c_r_4_1, f_c_r_5_1 = style_model(content_img,style_img)\n",
    "\n",
    "        f_c_r_4_1, f_c_r_5_1 = normalize_cw(f_c_r_4_1, f_c_r_5_1)\n",
    "        \n",
    "        pred_cs_r_4_1,pred_cs_r_5_1 = style_model.encoder(prediction)\n",
    "        pred_cs_r_4_1, pred_cs_r_5_1 = normalize_cw(pred_cs_r_4_1, pred_cs_r_5_1)\n",
    "\n",
    "        content_loss = CONTENT_WEIGHT*(torch.norm((pred_cs_r_4_1-f_c_r_4_1).view(BATCH_SIZE,-1), dim=-1).mean() + torch.norm((pred_cs_r_5_1-f_c_r_5_1).view(BATCH_SIZE,-1), dim=-1).mean())\n",
    "\n",
    "        _,_,style_features= style_loss_model((prediction,[],[]))\n",
    "\n",
    "        style_loss = 0.0\n",
    "        for f,f_target,weight in zip(style_features,style_features_target, STYLE_LAYERS_WEIGHTS):\n",
    "            f_mean = f.mean((2,3))\n",
    "            f_std = f.std((2,3))\n",
    "            f_target_mean = f_target.mean((2,3))\n",
    "            f_target_std = f_target.std((2,3))\n",
    "            style_loss += weight*(torch.norm(f_mean - f_target_mean,dim=-1).mean() + torch.norm(f_std - f_target_std,dim=-1).mean())\n",
    "\n",
    "        style_loss *= STYLE_WEIGHT\n",
    "\n",
    "        i_cc,_,_ = style_model(content_img,content_img)\n",
    "        i_ss,_,_ = style_model(style_img,style_img)\n",
    "\n",
    "        identity_1_loss = IDENTITY_1_WEIGHT*(torch.norm((i_cc-content_img).view(BATCH_SIZE,-1), dim=-1).mean() + torch.norm((i_ss-style_img).view(BATCH_SIZE,-1), dim=-1).mean())\n",
    "        _,_,i_cc_style_features= style_loss_model((i_cc,[],[]))\n",
    "        _,_,content_img_style_features= style_loss_model((content_img,[],[]))\n",
    "        _,_,i_ss_style_features= style_loss_model((i_ss,[],[]))\n",
    "        _,_,style_img_style_features= style_loss_model((style_img,[],[]))\n",
    "\n",
    "        identity_2_loss = 0.0\n",
    "        for f_icc,f_ic,f_iss,f_is in zip(i_cc_style_features,content_img_style_features,i_ss_style_features,style_img_style_features):\n",
    "            identity_2_loss += (torch.norm((f_icc-f_ic).view(BATCH_SIZE,-1), dim=-1).mean() + torch.norm((f_iss-f_is).view(BATCH_SIZE,-1), dim=-1).mean())\n",
    "        identity_2_loss *= IDENTITY_2_WEIGHT\n",
    "\n",
    "        content_loss_aggregator.append(content_loss.detach().cpu())\n",
    "        style_loss_aggregator.append(style_loss.detach().cpu())\n",
    "\n",
    "        loss = content_loss + style_loss + identity_1_loss + identity_2_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    optimimizer.step()  \n",
    "\n",
    "    if epoch%PRINT_STATS_EVERY == 0 and not epoch == 0:\n",
    "        print(\"Ending epoch: \", str(epoch), \" with content loss: \", torch.stack(content_loss_aggregator).mean().numpy().item(),  \" and style loss: \", torch.stack(style_loss_aggregator).mean().numpy().item())\n",
    "        ToPILImage()(content_img[0].permute(0,2,1)).save(SAVE_PATH + \"example_input_model.jpg\")\n",
    "        with torch.no_grad():   \n",
    "            img,_,_ = style_model(content_img[0].unsqueeze(0), style_img[0].unsqueeze(0))\n",
    "        img = ToPILImage()(img.cpu().squeeze(0).permute(0,2,1))\n",
    "        img.save(SAVE_PATH + \"example_output_model.jpg\")\n",
    "        torch.save(style_model.state_dict(), SAVE_PATH + \"model_weights.pth\")\n",
    "        torch.save(optimimizer.state_dict(), SAVE_PATH + \"optim_weights.pth\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "style_model.load_state_dict(torch.load(SAVE_PATH + \"model_weights.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "CONTENT_IMAGE_PATH = \"./dragon.jpg\"\n",
    "STYLE_IMAGE_PATH = \"./wave.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_img = Image.open(CONTENT_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "content_img = ImageOps.fit(content_img,(min(content_img.size),min(content_img.size))).resize(IMAGE_SIZE)\n",
    "content_img = ToTensor()(content_img).permute(0,2,1).to(device)\n",
    "\n",
    "style_img = Image.open(STYLE_IMAGE_PATH).convert('RGB')\n",
    "# center crop image\n",
    "style_img = ImageOps.fit(style_img,(min(style_img.size),min(style_img.size))).resize(IMAGE_SIZE)\n",
    "style_img = ToTensor()(style_img).permute(0,2,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "content_style_image,_,_ = style_model(content_img.unsqueeze(0),style_img.unsqueeze(0))\n",
    "content_style_image = ToPILImage()(content_style_image.squeeze(0).cpu().permute(0,2,1))\n",
    "content_style_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StyleTransfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
